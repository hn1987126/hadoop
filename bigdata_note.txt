apache旗下各软件下载
https://archive.apache.org/dist/

1、linux的两个命令：
#### 重启网络
/etc/rc.d/init.d/network restart

#### 小技巧
cd -  是回到上次的目录

#### 执行脚本前加上"."代表在当前进程执行如
. /bin/bash install.sh

#### 查看目录或文件所占空间单位智能化显示
du -sh *

#### tree 目录名   可以看他下面的所有子目录和文件，树状显示
tree /home/hadoop
需要yum -y install tree

#### linux查找，比如查找hadoop安装在哪了
find / -name "hadoop"

#### 批量查看某目录下所有子目录和子子目录
ll -R hdpdata    会列出hdpdata下的所有子目录和子子目录。

################# 特别好的小技巧，命令行里想让光标一次移动一个单词，可以设置一下.bashrc文件，这文件在每个用户的根目录下。
vi ~/.bashrc  在里面加入一行     set -o vi    即可，这时想按B和W把光标移到一个单词，可以按ESC，如果想切回，再按下ESC即可。
编辑完后需要执行下   source ~/.bashrc   使改变立即生效。

################# 文本编辑器技巧：：：
想要每行相同位置的字符进行统一修改，按住Alt键，再开始拖动，往上或往下拖，就可以批量修改。

################# 批量杀掉hadoop等进程：
killall -9 java

################# mac上启动mysql:
/usr/local/mysql/bin/mysql -u root -p123456

################# 设置mysql允许其他机器登录:
GRANT ALL ON *.* to root@'%' IDENTIFIED BY '123456';
FLUSH PRIVILEGES;

################# tail -f xxx.log 中 -f 与 -F 的区别：
-f 是监听指定的那文件的 iNode信息，就是文件本身，即使他重命名走了，也会监听他。
-F 他监听的就是纯文件名，不是iNode，重命名走了，就不管了，只找那文件名的文件。

################# Linux下软链接和硬链接的区别：
软链接：  记住软链接需要使用绝对路径，相对路径会出现能建成功，但是打不开的问题。
ln -s /home/hadoop/apps /aaa    这是在根目录下创建个aaa指向到 /home/hadoop/apps中
硬链接：
ln /home/hadoop/apps /abc
不需要-s参数，他链接的两文件是完全相同的文件，只不过是有两个inode。也就是说两个文件名指向同一个文件数据
一直要把这两文件都删除了，才算真的删除。如果只删了一个，因为还有别人在引用他，所以文件不会真的删除。

################# php里执行shell，这个shell需要以root的身份来执行。
可以修改 vim /etc/php-fpm.d/www.conf  这里有user和group都改成root
就代表以root来运行php，但是这样以后，启动php-fpm时就会出问题，正常 /usr/sbin/php-fpm是启动不了的，需要如下这样来启动：
nohup /usr/sbin/php-fpm -R >/dev/null 2>&1 &

################# 显示开启了哪些端口
netstat -tlpn

################# 查找文件
find / -name "nginx"
which nginx

################# shell中获取日期：date
昨天 date -d '-1 day'
格式化日期 date +%Y-%m-%d
使用别的日期，如上面这个日期赋值给一个变量：day_01=`date +%Y-%m-%d`;
year=`date --date=$day_01 +%Y`

################# 文本查找
grep -r 16010 /export/Domains/     查找这个目录和子目录下所有包含16010的行

################# 文本替换
sed -i "s/172.27.13.92/117.122.241.223/g" `grep 172.27.13.92 -rl /export/App`
sed -i "s/1qaz@WSX123/root@1234/g" `grep 1qaz@WSX123 -rl /export/App`

################# 批量结束tomcat进程
ps -ef | grep tomcat | awk '{print $2}' | xargs kill




#### keepalived 虚拟IP,实现高可用，管nginx集群的

####添加环境变量：
vi /etc/profile
export JAVA_HOME=/usr/local/java
export PATH=$PATH:$JAVA_HOME/bin

####文件操作：
取环境变量，用:隔开，取第一列，-f 1,3是取1和3列
echo $PATH | cut -d ':' -f 1
排序：用":"分隔，按第3列排序,这是按字符中排，如果是数字排则加n,-k 3n,如果要倒序排 -k 3nr
cat /etc/passwd | sort -t ':' -k 3

uniq 是去重，但一般是跟sort联合用，因为他只能作用于排好序的情况
cat /etc/passwd | sort | uniq
cat /etc/passwd | sort | uniq -c   能统计出重复的次数，结果为
1 admin
2 mysql

wc -w /etc/passwd  统计单词出现次数，空隔隔开的算
wc -l /etc/passwd  统计行数
wc -m /etc/passwd  统计文件字符数

#### sed执行命令操作，主要是对vi里面的吧
sed '2d' a.txt   删除a.txt的第二行，但是a.txt不变，只是把执行后的结果，删除第二行后，a.txt的结果，输出来。

#### awk
last -n 5  显示最后登录的5个人
last -n 5 | awk '{print $1}'    取第一列，默认以空隔来作为列分隔符
last -n 5 | awk -F ':' '{print $1}'   取第一列，以:为分隔符


########## 自动化部署脚本：带人机交互，ssh免密登录，输入密码。
# yum -y install expect    人机交互需要yum这个包
boot.sh   内容如下：
#!/bin/bash
SERVERS="s2 s3 s4"
PASSWORD=iloveme
BASE_SERVER=s1
auto_ssh_copy_id() {
    expect -c "set timeout -1;
        spawn ssh-copy-id $1;
        expect {
            *(yes/no)* {send -- yes\r;exp_continue;}
            *assword:* {send -- $2\r;exp_continue;}
            eof        {exit 0;}
        }";
}
ssh_copy_id_to_all() {
    for SERVER in $SERVERS
    do
        auto_ssh_copy_id $SERVER $PASSWORD
    done
}
ssh_copy_id_to_all
for SERVER in $SERVERS
do
    scp install.sh root@$SERVER:/root
    ssh root@$SERVER /bin/bash /root/install.sh
done

install.sh内容如下：
#!/bin/bash
BASE_SERVER=s1

#cat >> /etc/profile << EOF
#export JAVA_HOME=/usr/local/jdk1.7.0_45
#export PATH=\$PATH:\$JAVA_HOME/bin
#EOF


############ rz、sz需要的yum包
yum install lrzsz

############ zookeeper安装
cd /home/hadoop
tar -vxzf soft/zookeeper-3.4.5.tar.gz -C apps
cd apps/zookeeper-3.4.5
rm -rf src/ *.xml *.txt docs dist-maven
cd conf
cp zoo_sample.cfg zoo.cfg
vi zoo.cfg   修改dataDir为 /home/hadoop/zkdata
在zoo.cfg中新加：
server.1=s1:2888:3888
server.2=s2:2888:3888
server.3=s3:2888:3888
#2888是leder和foller通讯的端口，3888是投票端口

######### 上面的这个vi用如下命令执行
sed -i "/^dataDir=/c\dataDir=/home/hadoop/zkdata" /home/hadoop/apps/zookeeper-3.4.5/conf/zoo.cfg
servers="server.1=s1:2888:3888\n"
servers=$servers"server.2=s2:2888:3888\n"
servers=$servers"server.3=s3:2888:3888"
echo -e $servers >> /home/hadoop/apps/zookeeper-3.4.5/conf/zoo.cfg

mkdir -p /home/hadoop/zkdata
cd /home/hadoop/zkdata
echo 1 >myid

---------------拷到其他机器中
cd /home/hadoop/apps
scp -r zookeeper-3.4.5/ s2:/home/hadoop/apps
scp -r zookeeper-3.4.5/ s3:/home/hadoop/apps
在其他机器上:
cd /home/hadoop
mkdir zkdata
echo 2 >zkdata/myid

启动服务
/home/hadoop/apps/zookeeper-3.4.5/bin/zkServer.sh start
查看自己是不是leader
/home/hadoop/apps/zookeeper-3.4.5/bin/zkServer.sh status
必须要里面显示leader或follower才证明是成功
用jps查看  能看到 QuorumPeerMain


#关防火墙
/etc/init.d/iptables stop
chkconfig iptables off

#### zookeeper怎么用：？
/home/hadoop/apps/zookeeper-3.4.5/bin/zkCli.sh  进入命令行
ls /  查看所有节点
create /app1 "jiedianneirong"    创建持久的节点，如果加-e(create -e xxx)是创建临时节点。如果加-s参数，则在节点名字后自动加了个编号序号如创建app1，得最终的节点名是app1000000001这样的。
ls /  就能看到app1了
create /app1/server1 server1neirong
ls /app1   就能看到server1了
get /app1   能看到app1的文件内容及metedata信息(czxid,,mzxid,pzxid等内部事务号，版本号等)
set /app1/server1 hahaha   设置内容
del /app1/server1   删除节点，如果还有子节点则不让删
rmr /app1     删除app1和他的所有子节点递归

######## 通过ssh去另外一台机器执行命令
ssh s2 mkdir /root/test
# 远程启动服务
ssh s2 "source /etc/profile;/home/hadoop/apps/zookeeper-3.4.5/bin/zkServer.sh start"
# jps查看是否启动了zookeeper
输入jps  如果出现QuorumPeerMain 则代表启动了，没有则没启动

######## 批量启动zookeeper脚本：startzk.sh
#!bin/sh
echo "start zkServer..."
for i in 1 2 3
do
ssh s$i "source /etc/profile ; /home/hadoop/apps/zookeeper-3.4.5/bin/zkServer.sh start"
done



################  大数据
#### flume 日志往hadoop里写的工具
#### sqoop hive里的数据导入到mysql中，或mysql到hive，互导的工具
#### oozie 任务调度，训象师

################ 大数据环境搭建：
首先在s1,s2,s3,s4上批量新建hadoop用户
groupadd hadoop
useradd hadoop -g hadoop
passwd hadoop

给hadoop用户加sudo权限（今后就可以用sudo加原本的命令执行所有root的权限）
修改/etc/sudoers 这文件，加一行
hadoop  ALL=(ALL)       ALL
然后scp拷到各台机器
scp /etc/sudoers s2:/etc/
scp /etc/sudoers s3:/etc/
scp /etc/sudoers s4:/etc/
然后可以这样执行了 sudo hostname

######修改主机名,这一步是必须的，不然的话hadoop启动datanode会出错
vi /etc/sysconfig/network   分别改为s1,s2,s3,s4
这个是指临时修改 hostname s4
再用hostname查看，或用uname -n 查看
还有要在每台机器上/etc/hosts里加上：
192.168.80.128 s1
192.168.80.129 s2
192.168.80.130 s3
192.168.80.131 s4
注意原来默认的127.0.0.1 localhost那一段需要删除掉，不然取主机名还是会得到localhost

############### 下载hadoop
http://archive.apache.org/dist/   这是apache所有软件的各版本归档的地址
点hadoop进去，再点common是hadoop的各个版本包
注意：hadoop-src是指源码包，可以下载下来编译。
也可以直接用别人编译好的包。

############### 解压并修改配置文件
把hadoop-2.6.4.tar.gz包放在hadoop 这个用户的根目录下。
cd /home/hadoop
mkdir apps
tar -vxzf hadoop-2.6.4.tar.gz -C apps
cd apps/hadoop-2.6.4
rm -rf share/doc/*
cd etc/hadoop
#修改配置文件，一共4个
vim hadoop-env.sh   修改里面有个export JAVA_HOME 把他修改成自己的jdk目录
vim core-site.xml   核心配置
vim hdfs-site.xml
vim mapred-site.xml.example
vim yarn-site.xml

在core-site.xml中<configuration>里加入：
<property>
<name>fs.defaultFS</name>
<value>hdfs://s1:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/home/hadoop/hdpdata</value>
</property>


在hdfs-site.xml中<configuration>里加入：
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
<!--这是为了启动secondaryNameNode时，不从0.0.0.0(本机)上起，从那上起会很慢-->
<property>
<name>dfs.secondary.http.address</name>
<value>s1:50090</value>
</property>
<!--设置hdfs的namenode在多块磁盘上，每个同完全相同的东西，这个不是必须的。-->
<!--datanode也可以指定多块磁盘，设置dfs.data.dir属性，往不同的地方存，并发时效率高，各个磁盘并不存相同的东西，只是为了方便写操作-->
<property>
<name>dfs.name.dir</name>
<value>/home/hadoop/name1,/home/hadoop/name2</value>
</property>


mv mapred-site.xml.example vim mapred-site.xml
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>

在yarn-site.xml中<configuration>里加入：
<property>
<name>yarn.resourcemanager.hostname</name>
<value>s1</value>
</property>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>

#将hadoop添加到环境变量：
sudo vim /etc/profile
export HADOOP_HOME=/home/hadoop/apps/hadoop-2.6.4
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
拷到其他机器
sudo scp /etc/profile s2:/etc/
sudo scp /etc/profile s3:/etc/
sudo scp /etc/profile s4:/etc/


#格式化namenode
hadoop namenode -format
怎样算格式化成功呢？
-执行完的时候有提示 /home/hadoop/hdpdata/dfs/name has been successfully formatted.

#启动：
/home/hadoop/apps/hadoop-2.6.4/sbin/hadoop-daemon.sh start namenode
也可以直接hadoop-daemon.sh start namenode ,因为已经加了环境变量把sbin加进去了
然后用jps查看看是否有NameNode
通过网页访问(hadoop里内置了一个小的web应用服务器[jetty服务器]，他的默认端口是50070,http://s1:50070)
查看当前有多少空间，通过点击上方的overview标签查，地址为http://s1:50070/dfshealth.html#tab-overview


#####hadoop用户的免密登录
ssh-kengen
ssh-copy-id -i ~/.ssh/id_rsa.pub s1
ssh-copy-id -i ~/.ssh/id_rsa.pub s2
ssh-copy-id -i ~/.ssh/id_rsa.pub s3
ssh-copy-id -i ~/.ssh/id_rsa.pub s4

#把hadoop放到其他机器
cd /home/hadoop
tar -vczf apps.tar.gz apps
scp /home/hadoop/apps.tar.gz s2:/home/hadoop
ssh s2 tar -vxzf /home/hadoop/apps.tar.gz
scp /home/hadoop/apps.tar.gz s3:/home/hadoop
ssh s3 tar -vxzf /home/hadoop/apps.tar.gz
scp /home/hadoop/apps.tar.gz s4:/home/hadoop
ssh s4 tar -vxzf /home/hadoop/apps.tar.gz

#启动datanode，在其他机器上
hadoop-daemon.sh start datanode
hadoop-daemon.sh stop datanode  停止服务
#其他机器的datanode是怎么跟本机的namenode握手的呢？是因为hadoop目录连同配置文件一起拷过去了，他们都认识namenode为s1
日志文件是在：/home/hadoop/apps/hadoop-2.6.4/logs/目录下的 那个.log文件，而不是.out文件
namenode机器上jps查看，有namenode,还有个secondaryNameNode(这个就是想本机的0.0.0.0,这个慢，也可以在hdfs-site.xml中加dfs.secondary.http.address这个属性为s1:50090)
而datanode机器上jps查看只有datanode

#批量启动：
sbin/start-all.sh起所有(不建议)   start-dsf.sh只起dsf     start-yarn.sh只起yarn
批量启动哪些？在etc/hadoop/slaves中，这文件纯粹是给批量启动用的
vim slaves里加入（默认里面有个localhost，把这个去掉，改成底下的）
s2
s3
s4

#批量停止
sbin/stop-dsf.sh

#启动yarn
sbin/yarn-daemon.sh start resourcemanager
其他机器
sbin/yarn-daemon.sh start nodemanager


##### hadoop shell命令操作
hadoop fs -ls /     查看   或者  hdfs dfs -ls /
hadoop fs -put 1.txt /     上传到hdfs的根目录下
hadoop fs -get /1.txt      下载文件保存到当前目录下
hadoop fs -cat /1.txt      读取文件根目录下的1.txt
hadoop fs -mkdir -p /wordcount/input   在根目录下创建一个二级目录
hadoop fs -put a.txt b.txt /wordcount/input     上传多个文件
hdfs dfsadmin -report     查看datanode节点的情况，跟在s1:9000这个web上看到的是一样的，但会比他更实时准确
其他命令：
moveFromLocal 从本地剪切到hdfs  moveFromLocal a.txt /hd/
moveToLocal  从hdfs剪切到本地  moveToLocal /hd/a.txt /home/hadoop
appendToFile 追加一个文件到hdfs某个文件的末尾
getmerge     合并hdfs里的文件
rm           删除文件或文件夹
hadoop fs -du -s -h hdfs://s1:9000/*   统计文件夹的大小信息
hadoop fs -count /aaa/               统计指定目录下的文件节点数量
hadoop fs -setrep 3 /1.txt           改变a.txt这个文件的副本数（默认2个，在配置文件里配置的2个）

## hadoop目录所在：
cd /home/hadoop/hdpdata/dfs/data/current/BP-xxxxx/current/finalized/subdir0/subdir0 这样的目录下
的 blk_xxx文件，默认128M才会切成多个块，可以直接cat blk_xxx来查看刚才保存的1.txt的内容。
同时在其他机器上也有这样的文件。

#### 执行mapreduce去统计单词wordcount
hadoop jar /home/hadoop/apps/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar wordcount /wordcount/input /wordcount/output
--其中wordcount是要执行的这个包里的方法名，这方法需要两参数：输入和输出，要求output这个目录不存在，要是存在的话会报错
可以查看计算的结果
hadoop fs -cat /wordcount/output/part-r-00000


################### 讲解NameNode和SecondaryNameNode的关系
首先明白两个概念：
1、edits是记录操作日志的
2、fsimage是内存落盘到的目录，也就是镜像文件。
应该定期把edits和fsimage合并一下，生成新的fsimage，这样以后合并的时候不至于特别慢。
合并完后，要删掉edits或至少只留下未合并的edits。
正在写的edits是edits_inprogress，非inprogress的edits是非正在写的。
这个事不能在namenode上去做，而是要在secondnameNode上做。
namenode会定期(默认是30分钟)或edits到达一定的量时，会发请求给secondnameName，让他来合并。
secondnamenode会把namenode里的fsimage和一大堆edits下载到他所在的机器，
加载fsimage到内存，根据元数据生成的算法，在内存中重放一遍，再把内存dump到fsimage.chkpoint中。
上传到namenode服务器，并请求覆盖原来的fsimage（rename fsimage.checkpoint为fsimage）

########
hdpdata目录下的dfs有两个子目录，name和namesecondary，如果name被删除了，也可以拿namesecondary复制一份命名为name，也可以再用，只是会丢失一些最新的没来的及落盘的数据。

###########datanode的默认超时时间是10分30秒。(掉线或kill掉namenode)
由这两参数确定heartbeat.recheck.interval 默认为5分钟，里面是存毫秒单位
dfs.heartbeat.interval  默认为3，存的是秒单位
超时时间的计算公式是 2*heartbeat.recheck.interval + 10*dfs.heartbeat.interval

########### hadoop开发要引入的包
我把从apache官网上下载下来的hadoop包里的share/hadoop目录拷贝出来到跟项目平级的目录叫 lib_third
在idea工具中，File->Project Structure里Libraies中加入jar包，有如下是需要加入的：
1、lib_third/hadoop/common目录里的hadoop-common-2.6.4.jar  及它底下lib里的所有。
2、lib_third/hadoop/hdfs目录里的hadoop-hdfs-2.6.4.jar  及它底下lib里的所有。
3、lib_third/hadoop/mapreduce目录里的除hadoop-mapreduce-examples-2.6.4.jar之外的所有  及它底下lib里的所有。
4、lib_third/hadoop/yarn目录里的除包含server之外的所有  及它底下lib里的所有。

########### 用程序实现统计单词（MapReduces）
在服务器上任意s1,s2,s3,s4上执行：
hadoop fs -mkdir -p /wordcount/input
hadoop fs -rm -r /wordcount/output  需要删除output，不然会报错
# 把这几个现成的文件上传到hdfs
cd /home/hadoop/apps/hadoop-2.6.4
hadoop fs -put NOTICE.txt LICENSE.txt README.txt /wordcount/input
# 执行打好包的jar   java -cp jar包名 主类名 参数1 参数2（参数是在主类main中指定要接收的参数）
java -cp wordcount.jar cn.jhsoft.bigdata.hadoop.mr.wcdemo.WordcountDriver /wordcount/input /wordcount/output
#但是上面这样执行，如果wordcount.jar里没包含hadoop那一大堆包的话，就会报错，需要下面这样执行.它会把所有hadoop依赖包含进来。
hadoop jar wordcount.jar cn.jhsoft.bigdata.hadoop.mr.wcdemo.WordcountDriver /wordcount/input /wordcount/output

############ MapReduces分区，也就是结果按不同的要求生成统计结果
MapReduces里有个组件叫Partition，他是负责Map Task跑的时候，哪些单词进哪个分区，（后面一步就是分区里的进Reduces）
他的默认实现原理是，把key也就是单词进行hashCode再取模。取模是有几个Reduces就 %几，这样保证一个Reduces得到一个分区。
如果流量统计项目中，结果想按省来分出不同的输出结果，或者说在单词统计项目中，想要A-F一个统计文件，F-G一个文件，就需要对Partition组件进行重写。

#### MapReduces其他
number of split : 3 是指map有三个切片。其实大概就是 /wordcount/input里有几个文件大概就是几个切片。当然还得根据minBlock和maxBlock来。
MapReduces可以通过web页面来查看他的执行情况：http://s1:8088/

############ MapReduces中，Combiner组件的作用：
在Map Task进行时，生成<单词a,数量1>，<a,1>,<a,1>,<b,1>,<b,1>这样一个一个，很费空间还有传输的时候也影响速度。
Combiner组件的任务是在使这样的生成，进行合并，如<a,3>,<b,2>，实质上Combiner也是继承自Reducer，此类的实现与WordcountReducer完全相同，所以可以用它来作为Combiner。
但这也只适合能合并的情况，如果是一些运算类的，不合适合并就不要在这合并了，还是像原来一样在最后合并。

############ MapReduces合适处理大文件，最好hdfs里存储的就是大文件，怎么弄？
->这在数据源那进行控制，如把多个日志合并了，再往hdfs里写。但是如果是一些老数据，或无法合并，怎么弄？
小文件性能差，因为一个小文件就会有一个切片
->那就用job.setInputFormatClass(CombineTextInputFormat.class);在处理的时候进行逻辑合并。
这样就能少几个切片，

########### MapReduces运行时，Job所需的资源有：job.split, job.xml, jar这三个文件。

########### MapReduces数据倾斜
在做订单与商品做join on 的时候，发现有这样一个问题，因为他是拿 商品ID(pid)作为key的，这样在Reduces Task的时候，如果一个商品的订单特别多，如订单有10000个，而另外的商品只有2个订单。
这样因为默认 MapReduces是用key来hash来确定他是在哪个Reduces上跑。就导致一个Reduces特别忙，另外一个特别闲，这样就特别不均衡。就叫数据倾斜。
-》如何解决？
思路就是在Map里处理完，不到Reduces，Map里是按切片来的，还算很均衡。
订单表数据多，商品表数据少。这样就可以在Map Task里，每条订单，去MapReduces提供的缓存里取数据。取完直接拼接上，就出结果了，不用经过Reduce Task。
******* 这个就是通过 DistributedCache[分布式的缓存]来解决。
******* hadoop在本地跑出来的目录是 /tmp/hadoop-chen/mapred/local/xxx

########### MapReduces所有项目的思路：：
****先想清楚谁是key,想到相同的key会进同一个Reduces

########### MapReduces想把结果写到数据库或自定义结果文件的文件名：：用自定义job.setOutputFormatClass
########### 关于小文件的问题，之前想到两种办法，一种是在数据集成之前先弄成大文件，再放到hdfs，另外一种方法job.setInputFormatClass(CombineTextInputFormat.class);来合并
还有一种办法是：用MapReduces，把小文件弄成k,v形式，k为文件名，v为文件内容。实例代码在 cn.jhsoft.bigdata.hadoop.mr.combinefile 包中。

########### MapReduces全局计数器：每台机器每个Task，这里会全局来计数：（logenhance项目中有用到）
// 获取一个计数器用来记录不合法的日志行数, 组名malformed, 计数器名称malformedline
Counter counter = context.getCounter("malformed", "malformedline");
counter.increment(1); // 计数器加1
或者用枚举形式定义计数器，建议用上面的的字符串形式
enum MyCounter {MALFORORMED, NORMAL}
context.getCounter(MyCounter.MALFORORMED).increment(1);

########### MapReduces优化参数：
shuffle性能优化：
mapreduce.task.io.sort.mb   100    //shuffle的环形缓冲区大小，默认100m,可以调大
mapreduce.map.sort.spill.percent  0.8    // 环形缓冲区溢出的闸值，默认80%




########################################################################################
########################################################################################
### Hive安装
########################################################################################
########################################################################################
### hive装一台或多台都行，不需要集群，连同一个mysql就行。
1、apache-hive-1.2.1-bin.tar.gz
2、解压到apps目录里
cd /home/hadoop
tar -vxzf apache-hive-1.2.1-bin.tar.gz -C apps
3、配置文件
cd apps
mv apache-hive-1.2.1-bin/ hive

cd hive
# 创建一个新的配置文件：
vi conf/hive-site.xml
加入：
<configuration>
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
<description>JDBC connect string for a JDBC metastore</description>
</property>

<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
<description>Driver class name for a JDBC metastore</description>
</property>

<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>root</value>
<description>username to use against metastore database</description>
</property>

<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>123456</value>
<description>password to use against metastore database</description>
</property>
</configuration>

#### 还需要把mysql-connector-java-5.1.28.jar包放到 hive/lib目录下

4、启动
/home/hadoop/apps/hive/bin/hive

5、启动问题：
Jline包版本不一致的问题，需要拷贝hive的lib目录中jline.2.12.jar的jar包替换掉hadoop中的
cp /home/hadoop/apps/hive/lib/jline-2.12.jar /home/hadoop/apps/hadoop-2.6.4/share/hadoop/yarn/lib/

****创建了库之后，在hdfs的网页上就能看到 数据库的目录了 也就是在hdfs根目录下的这个目录下  /user/hive/warehouse
****在mysql中也能看到新建了个hive库，里面有DBS表和TBLS表，能看到自己建的库表。

5.1 测试
create database jhsoft;
use jhsoft;
create table test(id bigint, username string) row format delimited fields terminated by ',';
然后在hdfs里put 文件到 /user/hive/warehouse/jhsoft.db/test 下。
如果建表出错：FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don't support retries at the client level.)
需要在mysql中   alter database hive character set latin1;

6.建表(默认是内部表)
create table trade_detail(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\t';
#建分区表
create table td_part(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by '\t';
#建外部表
create external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\t' location '/td_ext';
#创建一个跟ods_weblog_origin一样的表，只有表结构
create table tmp1 like ods_weblog_origin;

7.创建分区表
#普通表和分区表区别：有大量数据增加的需要建分区表
create table book (id bigint, name string) partitioned by (pubdate string) row format delimited fields terminated by '\t';

#分区表加载数据
load data local inpath './book.txt' overwrite into table book partition (pubdate='2010-08-22');
load data local inpath '/root/data.am' into table beauty partition (nation="USA");
select nation, avg(size) from beauties group by nation order by avg(size);

8、启动hive为前台服务：/home/hadoop/apps/hive/bin/hiveserver2
启动hive为后台服务：nohup /home/hadoop/apps/hive/bin/hiveserver2 1>/var/log/hiveserver.log 2>/var/log/hiveserver.err &

在另外一个终端用命令行登录启动的服务：
/home/hadoop/apps/hive/bin/beeline
在命令行里输入：     !connect jdbc:hive2://localhost:10000      默认端口是10000
用户名输入  hadoop      这个可以在set里修改，一般情况下默认就是这个hadoop也不需要修改。
密码输入空    进去的操作就跟在mysql命令行一样。
或者这个在一步搞定，适合写在启动文件中：：
/home/hadoop/apps/hive/bin/beeline -u jdbc:hive2://localhost:10000 -n hadoop

###########################################
**** 在shell脚本里执行hive命令：
/home/hadoop/apps/hive/bin/hive -e 'show databases'
/home/hadoop/apps/hive/bin/hive -f 'test.sql'


9、### 建表：
内部表
create table if not exists t (id int, name string) row format delimited fields terminated by ',';
外部表
create external table if not exists t2(id int, name string) row format delimited fields terminated by ',' location '/db';
分区表(在建表时只是指定partiotion，具体在导入时才会插入)
create table if not exists t3(id int,name string) partitioned by (province string) row format delimited fields terminated by ',';
分桶表
create table if not exists t4(id int,name string) clustered by (id) sorted by (id) into 4 buckets row format delimited fields terminated by ',';


10、### 导数据：
load data local inpath '/home/hadoop/t.sql' into table t;
load data local inpath '/home/hadoop/t.sql' overwrite into table t;
导分区表
load data local inpath '/home/hadoop/t3.sql' into table t3 partition(province='bj')
load data local inpath '/home/hadoop/t3.sql' into table t3 partition(province='sh')
分桶表数据导入的，还是会原样导，不会进行分桶。
只有那种从别的表查询出来，再插入分桶表，会自动启动reduces程序，这种会进行分桶。下面两句要一起执行才有用。
set hive.enforce.bucketing=true;     [或者 set mapred.reduce.tasks=4;]
insert into table t4 select id,name from t1 cluster by (id);
#设置变量,设置分桶为true, 设置reduce数量是分桶的数量个数
set hive.enforce.bucketing = true;
set mapreduce.job.reduces=4;


11、### 分区表
分区表的目录结构是：/user/hive/warehouse/jhsoft.db/t3/province=bj    和  province=sh  这两文件。
再select * from t3 的时候会是有两个文件的所有数据，而且还多个province字段。
还想查某个分区的？
select * from t3 where province='bj'
增加分区：
alter table t3 add partition(province='wh');
删除分区
alter table t3 drop partition(province='wh');
显示t3表的所有partition
show partitions t3;
查看建表情况
desc extended t4;
清空表
truncate table t4;

12、### hive命令行执行命令：
dfs -ls /;    查看根目录的文件  后面都需要;表示结束
### 内部表和外部表的区别：
除了目录所在位置不一样，还有就是在drop table的时候，他们的无数据都是清空的，
但是外部表他的目录还在文件还在，内部表目录文件也会删除掉。

13、插入数据：
####用别的表的数据来创建自己的表
create table t5
as
select * from t1;
####查询别的表的数据的数据到本表(已存在)：
insert into t5 select * from t1;
insert overwrite t5 select * from t1;
####查询结果保存到文件目录，可以是本地，也可以是hdfs
insert overwrite local directory '/home/hadoop/test' select * from t1;
insert overwrite directory '/db/' select * from t1;   远程

14、hive中的各种join
准备工作：：
准备数据1.txt
1,ght
2,jd
3,yidao
准备数据2.txt
1,ght
2,jd
4,sina
5,shijijiayuan
建表
create table if not exists a (id int, name string) row format delimited fields terminated by ',';
create table if not exists b (id int, name string) row format delimited fields terminated by ',';
导入数据
load data local inpath '/home/hadoop/1.txt' into table a;
load data local inpath '/home/hadoop/2.txt' into table b;
** 【inner join】  就是取交集
select * from a inner join b on a.id=b.id;
+-------+---------+-------+---------+--+
| a.id  | a.name  | b.id  | b.name  |
+-------+---------+-------+---------+--+
| 1     | ght     | 1     | ght     |
| 2     | jd      | 2     | jd      |
+-------+---------+-------+---------+--+


** 【left join】   把左边的全显示出来，右边没有的，也显示出来
select * from a left join b on a.id=b.id;
+-------+---------+-------+---------+--+
| a.id  | a.name  | b.id  | b.name  |
+-------+---------+-------+---------+--+
| 1     | ght     | 1     | ght     |
| 2     | jd      | 2     | jd      |
| 3     | yidao   | NULL  | NULL    |
+-------+---------+-------+---------+--+


** 【right join】  把右边的全显示出来，左边没有的，也显示出来
select * from a right join b on a.id=b.id;
+-------+---------+-------+---------------+--+
| a.id  | a.name  | b.id  |    b.name     |
+-------+---------+-------+---------------+--+
| 1     | ght     | 1     | ght           |
| 2     | jd      | 2     | jd            |
| NULL  | NULL    | 4     | sina          |
| NULL  | NULL    | 5     | shijijiayuan  |
+-------+---------+-------+---------------+--+


** 【full outer join】 两边的全出来，连不上的就用NULL显示
select * from a full outer join b on a.id=b.id;
+-------+---------+-------+---------------+--+
| a.id  | a.name  | b.id  |    b.name     |
+-------+---------+-------+---------------+--+
| 1     | ght     | 1     | ght           |
| 2     | jd      | 2     | jd            |
| 3     | yidao   | NULL  | NULL          |
| NULL  | NULL    | 4     | sina          |
| NULL  | NULL    | 5     | shijijiayuan  |
+-------+---------+-------+---------------+--+

** 【left semi】  【它就是inner join的查询结果】 只显示左边的，且字段就只有左边表里的字段，
select * from a left semi join b on a.id=b.id;
+-------+---------+--+
| a.id  | a.name  |
+-------+---------+--+
| 1     | ght     |
| 2     | jd      |
+-------+---------+--+

****知识：left join 与 left outer join是一样的。outer可写可不写。


### 练习1
准备数据3.txt
1,陈义,100
2,宫海涛,100
3,国宾,101
create table if not exists c (id int, name string, depid int) row format delimited fields terminated by ',';
load data local inpath '/home/hadoop/3.txt' into table c;
# 查询
select s1.name from c s1 left semi join c s2 on s1.depid=s2.depid and s2.name='陈义';
+----------+--+
| s1.name  |
+----------+--+
| 陈义       |
| 宫海涛      |
+----------+--+

select * from c s1 left join c s2 on s1.depid=s2.depid and s2.name='陈义';
+--------+----------+-----------+--------+----------+-----------+--+
| s1.id  | s1.name  | s1.depid  | s2.id  | s2.name  | s2.depid  |
+--------+----------+-----------+--------+----------+-----------+--+
| 1      | 陈义       | 100       | 1      | 陈义       | 100       |
| 2      | 宫海涛      | 100       | 1      | 陈义       | 100       |
| 3      | 国宾       | 101       | NULL   | NULL     | NULL      |
+--------+----------+-----------+--------+----------+-----------+--+

select * from c s1 right join c s2 on s1.depid=s2.depid and s2.name='陈义';
+--------+----------+-----------+--------+----------+-----------+--+
| s1.id  | s1.name  | s1.depid  | s2.id  | s2.name  | s2.depid  |
+--------+----------+-----------+--------+----------+-----------+--+
| 1      | 陈义       | 100       | 1      | 陈义       | 100       |
| 2      | 宫海涛      | 100       | 1      | 陈义       | 100       |
| NULL   | NULL     | NULL      | 2      | 宫海涛      | 100       |
| NULL   | NULL     | NULL      | 3      | 国宾       | 101       |
+--------+----------+-----------+--------+----------+-----------+--+

select * from c s1 inner join c s2 on s1.depid=s2.depid and s2.name='陈义';
+--------+----------+-----------+--------+----------+-----------+--+
| s1.id  | s1.name  | s1.depid  | s2.id  | s2.name  | s2.depid  |
+--------+----------+-----------+--------+----------+-----------+--+
| 1      | 陈义       | 100       | 1      | 陈义       | 100       |
| 2      | 宫海涛      | 100       | 1      | 陈义       | 100       |
+--------+----------+-----------+--------+----------+-----------+--+

select * from c s1 left semi join c s2 on s1.depid=s2.depid and s2.name='陈义';
select s1.name from c s1 right semi join c s2 on s1.depid=s2.depid and s2.name='陈义';


### 练习2
准备数据  4.txt
-- 人员，月份，数量
-- 想要统计出每人每月的数量和，及累计值，如1月为10，累计值为10，2月20，累计值为30(10+20),3月值为15，累计为45(10+20+15)

A,2015-01,5
A,2015-01,15
B,2015-01,5
A,2015-01,8
B,2015-01,25
A,2015-01,5
A,2015-02,4
A,2015-02,6
B,2015-02,10
B,2015-02,5

#创建表
create table t6(username string,month string,amount int)
row format delimited fields terminated by ',';
#导数据
load data local inpath '/home/hadoop/4.txt' into table t6;
# 查询一步一步来：
## 先查出每人每月累计的值：
select username,month,sum(amount) as sum_amount from t6 group by username,month;
+-----------+----------+-------------+--+
| username  |  month   | sum_amount  |
+-----------+----------+-------------+--+
| A         | 2015-01  | 33          |
| A         | 2015-02  | 10          |
| B         | 2015-01  | 30          |
| B         | 2015-02  | 15          |
+-----------+----------+-------------+--+

## 上面这个已经完成了第一个要求。在这个的基础上，完成第二个要求，思路是可以把这结果作为子表，再inner join 他自己。
select * from
(select username,month,sum(amount) as sum_amount from t6 group by username,month) as a
inner join
(select username,month,sum(amount) as sum_amount from t6 group by username,month) as b
on a.username = b.username
这个的结果会是：
+-------------+----------+---------------+-------------+----------+---------------+--+
| a.username  | a.month  | a.sum_amount  | b.username  | b.month  | b.sum_amount  |
+-------------+----------+---------------+-------------+----------+---------------+--+
| A           | 2015-01  | 33            | A           | 2015-01  | 33            |
| A           | 2015-01  | 33            | A           | 2015-02  | 10            |
| A           | 2015-02  | 10            | A           | 2015-01  | 33            |
| A           | 2015-02  | 10            | A           | 2015-02  | 10            |
| B           | 2015-01  | 30            | B           | 2015-01  | 30            |
| B           | 2015-01  | 30            | B           | 2015-02  | 15            |
| B           | 2015-02  | 15            | B           | 2015-01  | 30            |
| B           | 2015-02  | 15            | B           | 2015-02  | 15            |
+-------------+----------+---------------+-------------+----------+---------------+--+

再想得累加值，需要group by 人名和月份。同时sum(sum_amount)
select username,month,max(sum_amount) as sum_amount,sum(sum_amounts) as sum_sum_amount from

(
select a.username,a.month,a.sum_amount,b.sum_amount as sum_amounts from
(select username,month,sum(amount) as sum_amount from t6 group by username,month) as a
inner join
(select username,month,sum(amount) as sum_amount from t6 group by username,month) as b
on a.username = b.username
) as tmp

group by username,month;

这条sql将得到累加值的结果：
+-----------+----------+-------------+-----------------+--+
| username  |  month   | sum_amount  | sum_sum_amount  |
+-----------+----------+-------------+-----------------+--+
| A         | 2015-01  | 33          | 43              |
| A         | 2015-02  | 10          | 43              |
| B         | 2015-01  | 30          | 45              |
| B         | 2015-02  | 15          | 45              |
+-----------+----------+-------------+-----------------+--+

这个结果还差一点，最后一列不应该是都43，应该第一条是33，第二条再是43。所以那里应该加个条件：
应该b.month要小于等于a.month，这样sum(b.sum_amount)的时候，才会是取此月及此月之前的累计值。
改造后：
select username,month,max(sum_amount) as sum_amount,sum(sum_amounts) as sum_sum_amount from

(
select a.username,a.month,a.sum_amount,b.month as b_month,b.sum_amount as sum_amounts from
(select username,month,sum(amount) as sum_amount from t6 group by username,month) as a
inner join
(select username,month,sum(amount) as sum_amount from t6 group by username,month) as b
on a.username = b.username
) as tmp

where b_month<=month
group by username,month;
+-----------+----------+-------------+-----------------+--+
| username  |  month   | sum_amount  | sum_sum_amount  |
+-----------+----------+-------------+-----------------+--+
| A         | 2015-01  | 33          | 33              |
| A         | 2015-02  | 10          | 43              |
| B         | 2015-01  | 30          | 30              |
| B         | 2015-02  | 15          | 45              |
+-----------+----------+-------------+-----------------+--+


这sql在inner join那一层的时候写的太繁琐了，用了个tmp临时表，是为了方便看，实际上可以简写成如下这样：
select a.username,a.month,max(a.sum_amount) as sum_amount,sum(b.sum_amount) as sum_sum_amount from
(select username,month,sum(amount) as sum_amount from t6 group by username,month) as a
inner join
(select username,month,sum(amount) as sum_amount from t6 group by username,month) as b
on a.username = b.username
where b.month<=a.month
group by a.username,a.month
order by a.username,a.month;
+-------------+----------+-------------+-----------------+--+
| a.username  | a.month  | sum_amount  | sum_sum_amount  |
+-------------+----------+-------------+-----------------+--+
| A           | 2015-01  | 33          | 33              |
| A           | 2015-02  | 10          | 43              |
| B           | 2015-01  | 30          | 30              |
| B           | 2015-02  | 15          | 45              |
+-------------+----------+-------------+-----------------+--+




15、hive里可以自定义函数(通过java类打包成jar实现)  和  transform调python脚本。
-------------
UDF案例：
create table rat_json(line string) row format delimited;
load data local inpath '/home/hadoop/rating.json' into table rat_json;
java里要引入hive的包，创建类，继承自org.apache.hadoop.hive.ql.exec.UDF,重写方法public String evaluate(String field)
把项目打成jar包，在hive命令行里：
add JAR /home/hadoop/myudf.jar;
create temporary function parsejson as 'cn.jhsoft.bigdata.hadoop.hive.udf.ToProvince';
然后在底下就可以执行如下的sql语句了。其中temporary是自定义函数的关键词。

### 这个是先用java写parsejson函数：此函数返回来的是一行一行的数据，用\t隔开字段，这句sql的目的是把\t隔开的东西分别插入数据库中，用split这个内置的函数。
create table t_rating as
select split(parsejson(line),'\t')[0]as movieid,split(parsejson(line),'\t')[1] as rate,split(parsejson(line),'\t')[2] as timestring,split(parsejson(line),'\t')[3] as uid from rat_json limit 10;

-------或者直接用get_json_object这个内置函数。
内置jason函数，这个是读取数据库的行,$.movie是json里key的名字。
select get_json_object(line,'$.movie') as moive,get_json_object(line,'$.rate') as rate  from rat_json limit 10;

-----------
transform案例:

***********使用transform+python的方式去转换unixtime为weekday
先编辑一个python脚本文件
########python######代码
vi weekday_mapper.py
#!/bin/python
import sys
import datetime

for line in sys.stdin:
  line = line.strip()
  movieid, rating, unixtime,userid = line.split('\t')
  weekday = datetime.datetime.fromtimestamp(float(unixtime)).isoweekday()
  print '\t'.join([movieid, rating, str(weekday),userid])

************然后，将文件加入hive的classpath：
hive>add FILE /home/hadoop/weekday_mapper.py;
hive>create TABLE u_data_new as
SELECT
  TRANSFORM (movieid, rate, timestring,uid)
  USING 'python weekday_mapper.py'
  AS (movieid, rate, weekday,uid)
FROM t_rating;

select distinct(weekday) from u_data_new limit 10;



################# 数据采集辅助工具：
一、Flume
数据采集，聚合，传输到hdfs
可以采集mysql,kafaka,文件，日志等。
****可以玩一下nc，他是用来建立soket服务器的，yum -y install nc，在一个终端输入nc -l 6666 就开启了一个服务器
****在另外的终端 nc s1 6666  即可与对方通话，如他发的消息，对方就能看到。
## 安装：上传apache-flume-1.6.0-bin.tar.gz到服务器，解压。

## 配置文件：：：：：：：：：
这个软件里不需要配置任何他的配置文件，只要新加一些自己的配置文件，指定采集源。

vi conf/netcat-logger.conf新建一个配置文件：：：
conf/netcat-logger.conf 配置文件内容：
# Name the components on this agent
#给那三个组件取个名字
a1.sources = r1   //源
a1.sinks = k1    // 目标
a1.channels = c1  // 通道

# Describe/configure the source
#类型, 从网络端口接收数据,在本机启动, 所以localhost, type=spooldir采集目录源,目录里有就采
#类型还有netcat,spooldir,exec,avro
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink 看效果的方式，目标不写到hdfs，而是输出在屏幕上立即显示
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
#下沉的时候是一批一批的, 下沉的时候是一个个eventChannel参数解释：
#capacity：默认该通道中最大的可以存储的event数量
#trasactionCapacity：每次最大可以从source中拿到或者送到sink中的event数量
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

启动：
/home/hadoop/apps/flume/bin/flume-ng agent --conf /home/hadoop/apps/flume/conf --conf-file /home/hadoop/apps/flume/conf/netcat-logger.conf --name a1 -Dflume.root.logger=INFO,console &
参数解释：
--conf-file 是指我的采集方案文件的所在目录
--name a1   是启动agent的名称，上面的那个conf-file配置文件里写了a1，他们对应着
-Dflume.root.logger=INFO,console   这是控制日志输出的，给netcat-logger.conf里传的参数，可以不用管
启动后，会监听本机的44444端口。
******* 注意，上面那配置文件里写的bind  是localhost是代表着本机telnet才能连上，如果想让外面别的机器能telnet上他，需要改成本机的主机名，如s1

### 测试：：
在另外一个终端，telnet localhost 44444
也可能需要yum -y install telnet
连上后就可以发消息与对方通信了。
flume 里接收到的是这样的：Event: { headers:{} body: 31 0D  hi i love m,}
*** telnet中在输入界面想退出  ctrl+]   才可以，再输入quit就可以了。

##############################################################
### 采其他的数据源，只要指定其他的配置文件即可，如网盘里的配置文件：
如  spooldir-hdfs.conf  (监视目录)
启动:
/home/hadoop/apps/flume/bin/flume-ng agent -c /home/hadoop/apps/flume/conf -f /home/hadoop/apps/flume/conf/spool-logger.conf -n a1 -Dflume.root.logger=INFO,console &

配置文件跟netcat-logger.conf比，变化的是：：
指定 a1.sources.r1.type = spooldir
a1.sources.r1.spoolDir = /home/hadoop/flumespool
a1.sources.r1.fileHeader = true

需要提前建好 目录 /home/hadoop/flumespool
移动一个文件如1.txt到flumespool目录中，cp,vim等都可以，只要有新的文件生成，spool都会知道。
采集的时候，是一行一个event到flume中。采集完成后，会重命名为 1.txt.COMPLETED。
如果采集完成后，再复制个1.txt进来，采集时就会出错，因为他再生命名为 1.txt.COMPLETED的时候，就会重名。

################################################################
上面的例子中，都是把目标定在终端显示上，其实实际工作中肯定是要写到hdfs里的，还是之前一样的配置文件，只是要设置sinks这块的东西：
# Describe the sinks
a1.sinks.k1.type = hdfs
a1.sinks.k1.channel = c1
a1.sinks.k1.hdfs.path = /flume/events/%y-%m-%d/%H%M/
a1.sinks.k1.hdfs.filePrefix = events-
a1.sinks.k1.hdfs.round = true
#底下两行是对 %H%M 生效的，上面的path是时分生成目录，底下这两行是定义10分钟生成一个新目录
a1.sinks.k1.hdfs.roundValue = 10
a1.sinks.k1.hdfs.roundUnit = minute
#文件滚动周期 默认是30秒，这里为了测试变小一点改为3秒
a1.sinks.k1.hdfs.rollInterval = 3
#文件滚动大小限制(bytes)  默认是1024字节,测试改小为200字节
a1.sinks.k1.hdfs.rollSize = 200
#发生了多少个事件(event)滚动一下，默认是10
a1.sinks.k1.hdfs.rollCount = 5
#有多大的时候，滚动一次，默认是100
a1.sinks.k1.hdfs.batchSize = 100
#从本地取时间
a1.sinks.k1.hdfs.useLocalTimeStamp = true
#生成的文件类型，默认是Sequencefile[序列文件，数据流]，可用DataStream，则为普通文本，还可以用压缩的CompressedStream
a1.sinks.k1.hdfs.fileType = DataStream

#用上面这些替换到原来的   a1.sinks.k1.type = logger

############## 完成的配置文件 tail-hdfs.conf,  exec这种类型，执行tail命令的时候会写到hdfs
# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = exec
a1.sources.r1.command = tail -F /home/hadoop/log/test.log
a1.sources.r1.channels = c1

# 上面那一堆的sinks的放在这里。

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1

##########执行命令前的准备：
a、首先把这配置文件拷到 /home/hadoop/apps/flume/conf下
b、创建 log目录，和log目录下的test.log文件
mkdir /home/hadoop/log
touch /home/hadoop/log/test.log
c、不断的往test.log文件里写内容，执行如下shell
while true
do
echo 11111111111 >> /home/hadoop/log/test.log
sleep 0.5
done

d、执行tail -F test.log 看看，看了以后就可以关了，因为flume里会去执行这个命令。

#########完成
flume本身不需要更任何配置，只需要配置数据源和接收者这个配置文件。

########## 执行命令
/home/hadoop/apps/flume/bin/flume-ng agent -c /home/hadoop/apps/flume/conf -f /home/hadoop/apps/flume/conf/tail-hdfs.conf -n a1

########## 结果
可以在hdfs里的根目录下创建flume目录(自动生成)。最里面的文件有.tmp结尾的，代表正在写的。


################################################################  flume 多个agent多级串联
如果是多个agent连接，则需要一台agent往另一台agent里写数据。这时接收数据就需要用到 avro 这种类型了。
第一台，也就是发送者指定 avrc sink ,hostname 和 port得是第二台接收都的  host和port 如s2 4141
第二台，接收者，他的数据源得是avro ,里面的hostname和port得是他本机的host名和指定的端口号。
发送者(第一台)的sinks得如下设置
# Describe the sink
a1.sinks = k1
a1.sinks.k1.type = avro
a1.sinks.k1.channel = c1
a1.sinks.k1.hostname = s2
a1.sinks.k1.port = 4141
#一批两个事件
a1.sinks.k1.batch-size = 2

接收者(第二台)的sources
# Describe/configure the source
a1.sources.r1.type = avro
a1.sources.r1.channels = c1
a1.sources.r1.bind = 0.0.0.0
a1.sources.r1.port = 4141

第一台执行：
/home/hadoop/apps/flume/bin/flume-ng agent -c /home/hadoop/apps/flume/conf -f /home/hadoop/apps/flume/conf/tail-avro.conf -n a1 -Dflume.root.logger=INFO,console
第二台执行：
/home/hadoop/apps/flume/bin/flume-ng agent -c /home/hadoop/apps/flume/conf -f /home/hadoop/apps/flume/conf/avro-hdfs.conf -n a1 -Dflume.root.logger=INFO,console


二、Azkaban  工作流调度器
在hadoop领域常用的工作流调度器有Oozie,Azkanban,Cascading,Hamake等
一共有三个包：azkaban-web-server-2.5.0.tar.gz  azkaban-executor-server-2.5.0.tar.gz  azkaban-sql-script-2.5.0.tar.gz
其中 azkaban-sql-script-2.5.0.tar.gz 是数据库初始化脚本，azkaban的东西他自己要存在mysql中，需要服务器上装了mysql

--------------------安装
首先拷文件
cd /home/hadoop
mkdir azkaban
tar -vxzf soft/azkaban/azkaban-web-server-2.5.0.tar.gz -C azkaban
tar -vxzf soft/azkaban/azkaban-executor-server-2.5.0.tar.gz -C azkaban
tar -vxzf soft/azkaban/azkaban-sql-script-2.5.0.tar.gz -C azkaban
cd azkaban
mv azkaban-web-2.5.0/ server
mv azkaban-executor-2.5.0/ executor

--------------------准备工作
## 进入mysql界面
/usr/local/mysql/bin/mysql -uroot -p123456
create database azkaban;
use azkaban;
source /home/hadoop/azkaban/azkaban-2.5.0/create-all-sql-2.5.0.sql

##创建SSL配置证书
keytool -keystore keystore -alias jetty -genkey -keyalg RSA
需要输入密钥库口令  123456   输两次
底下的全部回车 代表Unknown
确认的时候输入   y
Enter key password for <jetty>    按回车，表示与上面的密码一样
完成，此时在当前目录下生成了一个
cp keystore /home/hadoop/azkaban/server

## 统一时区：
sudo cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime
如果cp的时候提示没Asia/Shanghai这文件，需要先生成，用交互式命令tzselect即可生成。这个需要用root来执行，
> tzselect
在交互式里分别选择5(亚洲),9(中国),1(Beijing Time),1(yes)。这样就生成了。

## 配置文件：
### 文件1、web-server里的配置文件，也就是server目录：
vi /home/hadoop/azkaban/server/conf/azkaban.properties
一共要修改这几个地方：
default.timezone.id=Asia/Shanghai
mysql.user=root
mysql.password=123456
jetty.password=123456
jetty.keypassword=123456
jetty.trustpassword=123456

### 文件2、 还需要修改conf/azkaban-users.xml 这个配置文件，加用户名密码：
在空白行处加入<user username="admin" password="admin" roles="admin,metrics"/>

### 文件3、执行服务器，也就是 executor 服务器的配置：
vi /home/hadoop/azkaban/executor/conf/azkaban.properties
要修改这些：
default.timezone.id=Asia/Shanghai
mysql.database=azkaban
mysql.user=root
mysql.password=123456


*****上面三个配置文件，直接用下面的命令就改了，不用去vi里一个一个改：：：：：
sed -i "/^default.timezone.id=/c\default.timezone.id=Asia/Shanghai" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i "/^mysql.user=/c\mysql.user=root" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i "/^mysql.password=/c\mysql.password=123456" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i "/^jetty.password=/c\jetty.password=123456" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i "/^jetty.keypassword=/c\jetty.keypassword=123456" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i "/^jetty.trustpassword=/c\jetty.trustpassword=123456" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i "/^user.manager.xml.file=/c\user.manager.xml.file=/home/hadoop/azkaban/server/conf/azkaban-users.xml" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i "/^jetty.keystore=/c\jetty.keystore=/home/hadoop/azkaban/server/keystore" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i "/^jetty.truststore=/c\jetty.truststore=/home/hadoop/azkaban/server/keystore" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i "/^web.resource.dir=/c\web.resource.dir=/home/hadoop/azkaban/server/web/" /home/hadoop/azkaban/server/conf/azkaban.properties
sed -i '/^<azkaban-users>/a\        <user username="admin" password="admin" roles="admin,metrics"/>' /home/hadoop/azkaban/server/conf/azkaban-users.xml

sed -i "/^default.timezone.id=/c\default.timezone.id=Asia/Shanghai" /home/hadoop/azkaban/executor/conf/azkaban.properties
sed -i "/^mysql.database=/c\mysql.database=azkaban" /home/hadoop/azkaban/executor/conf/azkaban.properties
sed -i "/^mysql.user=/c\mysql.user=root" /home/hadoop/azkaban/executor/conf/azkaban.properties
sed -i "/^mysql.password=/c\mysql.password=123456" /home/hadoop/azkaban/executor/conf/azkaban.properties
sed -i "/^executor.global.properties=/c\executor.global.properties=/home/hadoop/azkaban/executor/conf/global.properties" /home/hadoop/azkaban/executor/conf/azkaban.properties



######## 启动
/home/hadoop/azkaban/server/bin/azkaban-web-start.sh
/home/hadoop/azkaban/executor/bin/azkaban-executor-start.sh
如果启动没问题，应该加入到后台去执行
nohup /home/hadoop/azkaban/server/bin/azkaban-web-start.sh 1>/var/log/azkaban-web.log 2>/var/log/azkaban-web.err &
nohup /home/hadoop/azkaban/executor/bin/azkaban-executor-start.sh 1>/var/log/azkaban-executor.log 2>/var/log/azkaban-executor.err &

######## 如果启动时报错：
Invalid maximum heap size: -Xmx4G
The specified size exceeds the maximum representable size.
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
这需要vi /home/hadoop/azkaban/server/bin/azkaban-web-start.sh
把里面的AZKABAN_OPTS="-Xmx4G" 修改为  AZKABAN_OPTS="-Xmx512M"
这需要vi /home/hadoop/azkaban/executor/bin/azkaban-executor-start.sh
把里面的AZKABAN_OPTS="-Xmx3G" 修改为  AZKABAN_OPTS="-Xmx512M"

######## 结束azkaban进程
/home/hadoop/azkaban/server/bin/azkaban-web-shutdown.sh
/home/hadoop/azkaban/executor/bin/azkaban-executor-shutdown.sh
也可以批量kill掉进程
ps -ef | grep azkaban | awk '{print $2}' | xargs kill

######## 可以通过 https://s1:8443 访问
用户名密码都为admin

######## 测试
新建hdfs.job文件，内容如下。并且把他打成zip包。在网盘的azkaban_jobs.zip包里就有
#hdfs.job
type=command
command=/home/hadoop/apps/hadoop-2.6.4/bin/hadoop fs -mkdir /abcde
用这个命令打成zip包   zip hdfs.zip hdfs.job
在web界面新建一个项目Create Project
Upload刚才打的那压缩包
点Execute Flow 点Execute即可，执行完成，点Detail看详情。



三、sqoop 数据迁移工具
Hadoop和关系数据库服务器之间传送数据的工具。
mysql到hadoop(hive,hdfs,hbase)，hadoop到mysql关系型数据库

### 安装前的准备：
网盘里sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz这个包
还需要mysql驱动 mysql-connector-java-5.1.28.jar

cd /home/hadoop
tar -vxzf soft/sqoop-1.4.6.bin__hadoop-2.0.4-alpha.tar.gz -C apps
cd apps
mv sqoop-1.4.6.bin__hadoop-2.0.4-alpha/ sqoop
cd sqoop/conf
mv sqoop-env-template.sh sqoop-env.sh

#修改配置文件
sed -i "/^#export HADOOP_COMMON_HOME=/c\export HADOOP_COMMON_HOME=/home/hadoop/apps/hadoop-2.6.4/" /home/hadoop/apps/sqoop/conf/sqoop-env.sh
sed -i "/^#export HADOOP_MAPRED_HOME=/c\export HADOOP_MAPRED_HOME=/home/hadoop/apps/hadoop-2.6.4/" /home/hadoop/apps/sqoop/conf/sqoop-env.sh
sed -i "/^#export HIVE_HOME=/c\export HIVE_HOME=/home/hadoop/apps/hive" /home/hadoop/apps/sqoop/conf/sqoop-env.sh

#加入mysql的jdbc驱动包
cp /home/hadoop/apps/hive/lib/mysql-connector-java-5.1.28.jar /home/hadoop/apps/sqoop/lib

###### 启动：
/home/hadoop/apps/sqoop/bin/sqoop

###### 导入导出
export是从hdfs导到mysql中
import是从mysql到hdfs中
导入：(--m 1 是指用1个reduces来跑)
/home/hadoop/apps/sqoop/bin/sqoop import \
--connect jdbc:mysql://s1:3306/jhs_zhongchou \
--username root \
--password 123456 \
--table jhs_html \
--target-dir /db/queryresult \
--m 1

--target-dir 是指定hdfs的目录，如果不加则默认是导到/user/hadoop/目录下，此目录里生成文件jhs_html/part-m-00000中
hadoop fs -cat /user/hadoop/jhs_html/part-m-00000

#先把上面导的文件删掉
hadoop fs -rm -r /user/hadoop/jhs_html
导入到hive
/home/hadoop/apps/sqoop/bin/sqoop import \
--connect jdbc:mysql://s1:3306/jhs_zhongchou \
--username root \
--password 123456 \
--table jhs_html \
--hive-import \
--m 1

导入的过程中，加where条件，把查询结果导入
/home/hadoop/apps/sqoop/bin/sqoop import \
--connect jdbc:mysql://s1:3306/jhs_zhongchou \
--username root \
--password 123456 \
--table jhs_html \
--where "id=1" \
--target-dir /db/queryresult \
--m 1

导入加query 查询，注意query后面的and $CONDITIONS是一定要加的，而且--table就一定不要要了，因为query里已经有表在了。
/home/hadoop/apps/sqoop/bin/sqoop import \
--connect jdbc:mysql://s1:3306/jhs_zhongchou \
--username root \
--password 123456 \
--query 'select id,title,url from jhs_html where id > 2 and $CONDITIONS' \
--target-dir /db/queryresult2 \
--m 1

######## 增量导入
这个语句 指明要增量导入，主键是id字段，上次导到了 3 这个id，本次就从1026开始导
/home/hadoop/apps/sqoop/bin/sqoop import \
--connect jdbc:mysql://s1:3306/jhs_zhongchou \
--username root \
--password 123456 \
--table jhs_html --m 1 \
--target-dir /db/queryresult3 \
--incremental append \
--check-column id \
--last-value 3

执行完后有如下这样的提示：
17/07/19 00:11:59 INFO tool.ImportTool:  --incremental append
17/07/19 00:11:59 INFO tool.ImportTool:   --check-column id
17/07/19 00:11:59 INFO tool.ImportTool:   --last-value 6


############ 导出
/home/hadoop/apps/sqoop/bin/sqoop export \
--connect jdbc:mysql://s1:3306/jhs_zhongchou \
--username root \
--password 123456 \
--table jhs_html \
--export-dir /db/queryresult3


##########################################################################
########## 点击流项目 过程
##########################################################################
1、数据清洗
2、数据预处理
3、数据仓库设计(星型模型  【事实表，维度表 组合关联查询】)(雪花模型  【与事实表关联的是 键，键再和维度表关联。按三范式设计】)
创建事实表和维度表。
4、ETL数据抽取，从源表抽取数据到目标表
5、统计分析

hive中row_number()的用法：
显示序号，row_number() over ()
通常用于取分组topN。
如库里有，男，有女。想在一条sql里分别取男的top10，和女的top10。
原来的做法只能按性别group by，再按所有人的年龄排序：
select sex,age from tt1 order by age desc group by sex;
hive里这样用： 取分组后的每组的前两个。



#测试数据：
1,男,25
2,男,18
3,男,32
4,女,40
5,女,35
6,男,32
7,女,45

#测试表
create table tt1 (id int,sex string,age int) row format delimited fields terminated by ',';
load data local inpath '/home/hadoop/1.txt' into table tt1;

#hql
select sex,age,row_number() over (partition by sex order by age asc) as od from tt1 ;
+------+------+-----+--+
| sex  | age  | od  |
+------+------+-----+--+
| 女    | 35   | 1   |
| 女    | 40   | 2   |
| 女    | 45   | 3   |
| 男    | 18   | 1   |
| 男    | 25   | 2   |
| 男    | 32   | 3   |
| 男    | 32   | 4   |
+------+------+-----+--+

想要取前2名的：
select * from
(select sex,age,row_number() over (partition by sex order by age asc) as od from tt1) tmp
where od<=2;
+----------+----------+---------+--+
| tmp.sex  | tmp.age  | tmp.od  |
+----------+----------+---------+--+
| 女        | 35       | 1       |
| 女        | 40       | 2       |
| 男        | 18       | 1       |
| 男        | 25       | 2       |
+----------+----------+---------+--+



项目：关键路径转化率分析，漏斗模型，借助shell目录下的python文件来生成访问日志mylog.log
创建表：
drop table if exists ods_weblog_origin;
create table ods_weblog_origin(
valid string,
remote_addr string,
remote_user string,
time_local string,
request string,
status string,
body_bytes_sent string,
http_referer string,
http_user_agent string)
row format delimited
fields terminated by '\001';

#导数据
load data local inpath "/home/hadoop/mylog.log" into table ods_weblog_origin;

#创建统计表
create table route_numbs as
select 'step1' as step,count(distinct remote_addr) as numbs from ods_weblog_origin where request like '/item%'
union
select 'step2' as step,count(distinct remote_addr) as numbs from ods_weblog_origin where request like '/category%'
union
select 'step3' as step,count(distinct remote_addr) as numbs from ods_weblog_origin where request like '/order%'
union
select 'step4' as step,count(distinct remote_addr) as numbs from ods_weblog_origin where request like '/index%';


#inner join自己连接自己
select a.step as a_step,a.numbs as a_numbs,b.step as b_step,b.numbs as b_numbs from route_numbs as a
inner join
route_numbs as b


#求相对第一步的转化率
select a_step,a_numbs,a_numbs/b_numbs as ratio from
(
select a.step as a_step,a.numbs as a_numbs,b.step as b_step,b.numbs as b_numbs from route_numbs as a inner join route_numbs as b
) tmp
where b_step='step1';
+---------+----------+----------------------+--+
| a_step  | a_numbs  |         ratio        |
+---------+----------+----------------------+--+
| step1   | 1015     | 1.0                  |
| step2   | 150      | 0.1477832512315271   |
| step3   | 89       | 0.08768472906403942  |
| step4   | 47       | 0.04630541871921182  |
+---------+----------+----------------------+--+


#求相对上一步的转化率
select a_step,a_numbs/b_numbs as ration from
(
select a.step as a_step,a.numbs as a_numbs,b.step as b_step,b.numbs as b_numbs from route_numbs as a inner join route_numbs as b
) tmp
where cast(substr(a_step,5,1) as int)=cast(substr(b_step,5,1) as int)+1;
+---------+---------------------+--+
| a_step  |       ration        |
+---------+---------------------+--+
| step2   | 0.1477832512315271  |
| step3   | 0.5933333333333334  |
| step4   | 0.5280898876404494  |
+---------+---------------------+--+



#把他们合并在一张表里：
select abs.a_step,abs.a_numbs,abs.ratio,rel.ration from
(
select a_step,a_numbs,a_numbs/b_numbs as ratio from
(
select a.step as a_step,a.numbs as a_numbs,b.step as b_step,b.numbs as b_numbs from route_numbs as a inner join route_numbs as b
) tmp
where b_step='step1'
) abs

left join

(
select a_step,a_numbs/b_numbs as ration from
(
select a.step as a_step,a.numbs as a_numbs,b.step as b_step,b.numbs as b_numbs from route_numbs as a inner join route_numbs as b
) tmp
where cast(substr(a_step,5,1) as int)=cast(substr(b_step,5,1) as int)+1
) rel

on abs.a_step=rel.a_step;

+-------------+--------------+----------------------+---------------------+--+
| abs.a_step  | abs.a_numbs  |      abs.ratio       |     rel.ration      |
+-------------+--------------+----------------------+---------------------+--+
| step1       | 1015         | 1.0                  | NULL                |
| step2       | 150          | 0.1477832512315271   | 0.1477832512315271  |
| step3       | 89           | 0.08768472906403942  | 0.5933333333333334  |
| step4       | 47           | 0.04630541871921182  | 0.5280898876404494  |
+-------------+--------------+----------------------+---------------------+--+





##########################################################################
########## hbase
##########################################################################
需要包：hbase-0.99.2-bin.tar.gz
tar -vxzf soft/hbase-0.99.2-bin.tar.gz -C apps
cd apps
mv hbase-0.99.2 hbase
cd hbase
rm -rf docs/

su - root
## 配置环境变量
cat >> /etc/profile << EOF
export HBASE_HOME=/home/hadoop/apps/hbase
export PATH=$PATH:$HBASE_HOME/bin
EOF

source /etc/profile
su - hadoop

# 修改配置文件hbase-env.sh
sed -i "/^# export JAVA_HOME=/c\export JAVA_HOME=/usr/local/java" /home/hadoop/apps/hbase/conf/hbase-env.sh
sed -i "/^# export HBASE_MANAGES_ZK=/c\export HBASE_MANAGES_ZK=false" /home/hadoop/apps/hbase/conf/hbase-env.sh
sed -i "/^# export HBASE_CLASSPATH=/a\export JAVA_CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar" /home/hadoop/apps/hbase/conf/hbase-env.sh

# 修改配置文件hbase-site.xml
sed -i "/configuration>/d" /home/hadoop/apps/hbase/conf/hbase-site.xml
cat >> /home/hadoop/apps/hbase/conf/hbase-site.xml << EOF
<configuration>
  <property>
      <name>hbase.master</name>		#hbasemaster的主机和端口
      <value>s1:60000</value>
  </property>
  <property>
      <name>hbase.master.maxclockskew</name>    #时间同步允许的时间差
      <value>180000</value>
  </property>
  <property>
      <name>hbase.rootdir</name>
      <value>hdfs://s1:9000/hbase</value>  #hbase共享目录，持久化hbase数据
  </property>
  <property>
      <name>hbase.cluster.distributed</name>  #是否分布式运行，false即为单机
      <value>true</value>
  </property>
  <property>
      <name>hbase.zookeeper.quorum</name>  #zookeeper地址
      <value>s1,s2,s3</value>
  </property>
  <property>
      <name>hbase.zookeeper.property.dataDir</name>   #zookeeper配置信息快照的位置
      <value>/home/hadoop/zkdata</value>
  </property>
</configuration>
EOF

#修改 regionservers文件
cat > /home/hadoop/apps/hbase/conf/regionservers << EOF
s2
s3
EOF

# 把hadoop的hdfs-site.xml和core-site.xml 放到hbase/conf下
cp /home/hadoop/apps/hadoop-2.6.4/etc/hadoop/hdfs-site.xml /home/hadoop/apps/hbase/conf
cp /home/hadoop/apps/hadoop-2.6.4/etc/hadoop/core-site.xml /home/hadoop/apps/hbase/conf

# 发送到其他机器
scp -r /home/hadoop/apps/hbase hadoop@s2:/home/hadoop/apps/
scp -r /home/hadoop/apps/hbase hadoop@s3:/home/hadoop/apps/

# 启动
/home/hadoop/apps/hbase/bin/start-hbase.sh

# 使用
进入hbase的shell：/home/hadoop/apps/hbase/bin/hbase shell
页面：http://s1:60010/
如果在使用中，出现Can't get master address from ZooKeeper; znode data == null，需要重启一下 hbase

### 命令行命令：
创建表       #create '表名', '列族名1','列族名2','列族名N'
create 'test1','namenode','datanode'
create 'test2','namenode','datanode'

查看所有表  list
描述表  describe 'test1'
判断表是否存在   exists  'test1'
判断表是否启用禁用   is_enabled 'test1'     或   is_disabled 'test1'
添加记录       put  '表名', 'rowKey(也就是此记录的标识)', '列族:列' ,  '值'
put 'test1','1001','namenode:relname','陈义'
put 'test1','1001','namenode:sex','1'
put 'test1','1001','namenode:age','30'
put 'test1','1001','datanode:city','bj'

查看 rowKey下的所有记录   get  '表名' , 'rowKey'
get 'test1','1001'
查看 记录总数  count 'test1'
获取某个列族   get '表名','rowkey','列族'
get 'test1','1001','namenode'

获取某个列族的某个列  get '表名','rowkey','列族：列’
get 'test1','1001','namenode:sex'

删除记录    delete  ‘表名’ ,‘行名’ , ‘列族：列'
delete 'test1','1001','namenode:sex'

删除整行                      deleteall '表名','rowkey'
deleteall 'test1','1001'

删除一张表    先要屏蔽该表，才能对该表进行删除，清空表第一步disable ‘表名’ ，第二步  drop '表名'
disable 'test2';
drop 'test2'

清空表     truncate '表名'
truncate 'test1'

查看某个表某个列中所有数据       scan "表名"
scan 'test1'
scan 'test1',{COLUMNS=>'namenode:sex'}

更新记录      scan "表名" , {COLUMNS=>'列族名:列名'}
scan 'test1',{COLUMNS=>'namenode:sex'}






##################
### storm需要python2.7版本，所以centos5.5下默认的python2.4需要升级。
##################
cd /home/hadoop/soft
wget http://www.python.org/ftp/python/2.7.3/Python-2.7.3.tgz
tar -zxvf Python-2.7.3.tgz

#在编译前先在/usr/local建一个文件夹python27（作为python的安装路径，以免覆盖老的版本）
su -
mkdir -p /usr/local/python27
cd /home/hadoop/soft/Python-2.7.3
./configure --prefix=/usr/local/python27
make && make install

#此时没有覆盖老版本，再将原来/usr/bin/python链接改为别的名字
mv /usr/bin/python /usr/bin/python_old
ln -s /usr/local/python27/bin/python2.7 /usr/bin/python
## 此时再输入python -V 即可看到2.7版本的了。



























