1、linux的两个命令：
#### 重启网络
/etc/rc.d/init.d/network restart
#### 小技巧
cd -  是回到上次的目录
#### 执行脚本前加上"."代表在当前进程执行如
. /bin/bash install.sh
#### 查看目录或文件所占空间单位智能化显示
du -sh *
#### tree 目录名   可以看他下面的所有子目录和文件，树状显示
tree /home/hadoop
需要yum -y install tree
#### linux查找，比如查找hadoop安装在哪了
find / -name "hadoop"
#### 批量查看某目录下所有子目录和子子目录
ll -R hdpdata    会列出hdpdata下的所有子目录和子子目录。
################# 特别好的小技巧，命令行里想让光标一次移动一个单词，可以设置一下.bashrc文件，这文件在每个用户的根目录下。
vi ~/.bashrc  在里面加入一行     set -o vi    即可，这时想按B和W把光标移到一个单词，可以按ESC，如果想切回，再按下ESC即可。
编辑完后需要执行下   source ~/.bashrc   使改变立即生效。
################# 文本编辑器技巧：：：
想要每行相同位置的字符进行统一修改，按住Alt键，再开始拖动，往上或往下拖，就可以批量修改。
################# 批量杀掉hadoop等进程：
killall -9 java
################# mac上启动mysql:
/usr/local/mysql/bin/mysql -u root -p123456
################# 设置mysql允许其他机器登录:
GRANT ALL ON *.* to root@'%' IDENTIFIED BY '123456';
FLUSH PRIVILEGES;


#### keepalived 虚拟IP,实现高可用，管nginx集群的

####添加环境变量：
vi /etc/profile
export JAVA_HOME=/usr/local/java
export PATH=$PATH:$JAVA_HOME/bin

####文件操作：
取环境变量，用:隔开，取第一列，-f 1,3是取1和3列
echo $PATH | cut -d ':' -f 1
排序：用":"分隔，按第3列排序,这是按字符中排，如果是数字排则加n,-k 3n,如果要倒序排 -k 3nr
cat /etc/passwd | sort -t ':' -k 3

uniq 是去重，但一般是跟sort联合用，因为他只能作用于排好序的情况
cat /etc/passwd | sort | uniq
cat /etc/passwd | sort | uniq -c   能统计出重复的次数，结果为
1 admin
2 mysql

wc -w /etc/passwd  统计单词出现次数，空隔隔开的算
wc -l /etc/passwd  统计行数
wc -m /etc/passwd  统计文件字符数

#### sed执行命令操作，主要是对vi里面的吧
sed '2d' a.txt   删除a.txt的第二行，但是a.txt不变，只是把执行后的结果，删除第二行后，a.txt的结果，输出来。

#### awk
last -n 5  显示最后登录的5个人
last -n 5 | awk '{print $1}'    取第一列，默认以空隔来作为列分隔符
last -n 5 | awk -F ':' '{print $1}'   取第一列，以:为分隔符


########## 自动化部署脚本：带人机交互，ssh免密登录，输入密码。
# yum -y install expect    人机交互需要yum这个包
boot.sh   内容如下：
#!/bin/bash
SERVERS="s2 s3 s4"
PASSWORD=iloveme
BASE_SERVER=s1
auto_ssh_copy_id() {
    expect -c "set timeout -1;
        spawn ssh-copy-id $1;
        expect {
            *(yes/no)* {send -- yes\r;exp_continue;}
            *assword:* {send -- $2\r;exp_continue;}
            eof        {exit 0;}
        }";
}
ssh_copy_id_to_all() {
    for SERVER in $SERVERS
    do
        auto_ssh_copy_id $SERVER $PASSWORD
    done
}
ssh_copy_id_to_all
for SERVER in $SERVERS
do
    scp install.sh root@$SERVER:/root
    ssh root@$SERVER /bin/bash /root/install.sh
done

install.sh内容如下：
#!/bin/bash
BASE_SERVER=s1

#cat >> /etc/profile << EOF
#export JAVA_HOME=/usr/local/jdk1.7.0_45
#export PATH=\$PATH:\$JAVA_HOME/bin
#EOF


############ rz、sz需要的yum包
yum install lrzsz

############ zookeeper安装
cd /root/soft
tar -vxzf zookeeper-3.4.5.tar.gz
cd /root
mkdir apps
mv soft/zookeeper-3.4.5 apps
cd /root/apps/zookeeper-3.4.5
rm -rf src/ *.xml *.txt docs dist-maven
cd conf
cp zoo_sample.cfg zoo.cfg
vi zoo.cfg   修改dataDir为 /root/zkdata
在zoo.cfg中新加：
server.1=s1:2888:3888
server.2=s2:2888:3888
server.3=s3:2888:3888
#2888是leder和foller通讯的端口，3888是投票端口
mkdir -p /root/zkdata
cd /root/zkdata
echo 1 >myid
---------------拷到其他机器中
cd /root/
scp -r apps/ s2:/root
在其他机器上:
cd /root
mkdir zkdata
echo 2 >zkdata/myid

启动服务
/root/apps/zookeeper-3.4.5/bin/zkServer.sh start
查看自己是不是leader
/root/apps/zookeeper-3.4.5/bin/zkServer.sh status
必须要里面显示leader或follower才证明是成功


#关防火墙
/etc/init.d/iptables stop
chkconfig iptables off

#### zookeeper怎么用：？
/root/apps/zookeeper-3.4.5/bin/zkCli.sh  进入命令行
ls /  查看所有节点
create /app1 "jiedianneirong"    创建持久的节点，如果加-e(create -e xxx)是创建临时节点。如果加-s参数，则在节点名字后自动加了个编号序号如创建app1，得最终的节点名是app1000000001这样的。
ls /  就能看到app1了
create /app1/server1 server1neirong
ls /app1   就能看到server1了
get /app1   能看到app1的文件内容及metedata信息(czxid,,mzxid,pzxid等内部事务号，版本号等)
set /app1/server1 hahaha   设置内容
del /app1/server1   删除节点，如果还有子节点则不让删
rmr /app1     删除app1和他的所有子节点递归

######## 通过ssh去另外一台机器执行命令
ssh s2 mkdir /root/test
# 远程启动服务
ssh s2 "source /etc/profile;/root/apps/zookeeper-3.4.5/bin/zkServer.sh start"
# jps查看是否启动了zookeeper
输入jps  如果出现QuorumPeerMain 则代表启动了，没有则没启动

######## 批量启动zookeeper脚本：startzk.sh
#!bin/sh
echo "start zkServer..."
for i in 1 2 3
do
ssh s$i "source /etc/profile ; /root/apps/zookeeper-3.4.5/bin/zkServer.sh start"
done



################  大数据
#### flume 日志往hadoop里写的工具
#### sqoop hive里的数据导入到mysql中，或mysql到hive，互导的工具
#### oozie 任务调度，训象师

################ 大数据环境搭建：
首先在s1,s2,s3,s4上批量新建hadoop用户
groupadd hadoop
useradd hadoop -g hadoop
passwd hadoop

给hadoop用户加sudo权限（今后就可以用sudo加原本的命令执行所有root的权限）
修改/etc/sudoers 这文件，加一行
hadoop  ALL=(ALL)       ALL
然后scp拷到各台机器
scp /etc/sudoers s2:/etc/
scp /etc/sudoers s3:/etc/
scp /etc/sudoers s4:/etc/
然后可以这样执行了 sudo hostname

######修改主机名,这一步是必须的，不然的话hadoop启动datanode会出错
vi /etc/sysconfig/network   分别改为s1,s2,s3,s4
这个是指临时修改 hostname s4
再用hostname查看，或用uname -n 查看
还有要在每台机器上/etc/hosts里加上：
192.168.80.128 s1
192.168.80.129 s2
192.168.80.130 s3
192.168.80.131 s4
注意原来默认的127.0.0.1 localhost那一段需要删除掉，不然取主机名还是会得到localhost

############### 下载hadoop
http://archive.apache.org/dist/   这是apache所有软件的各版本归档的地址
点hadoop进去，再点common是hadoop的各个版本包
注意：hadoop-src是指源码包，可以下载下来编译。
也可以直接用别人编译好的包。

############### 解压并修改配置文件
把hadoop-2.6.4.tar.gz包放在hadoop 这个用户的根目录下。
cd /home/hadoop
mkdir apps
tar -vxzf hadoop-2.6.4.tar.gz -C apps
cd apps/hadoop-2.6.4
rm -rf share/doc/*
cd etc/hadoop
#修改配置文件，一共4个
vim hadoop-env.sh   修改里面有个export JAVA_HOME 把他修改成自己的jdk目录
vim core-site.xml   核心配置
vim hdfs-site.xml
vim mapred-site.xml.example
vim yarn-site.xml

在core-site.xml中<configuration>里加入：
<property>
<name>fs.defaultFS</name>
<value>hdfs://s1:9000</value>
</property>
<property>
<name>hadoop.tmp.dir</name>
<value>/home/hadoop/hdpdata</value>
</property>


在hdfs-site.xml中<configuration>里加入：
<property>
<name>dfs.replication</name>
<value>2</value>
</property>
<!--这是为了启动secondaryNameNode时，不从0.0.0.0(本机)上起，从那上起会很慢-->
<property>
<name>dfs.secondary.http.address</name>
<value>s1:50090</value>
</property>
<!--设置hdfs的namenode在多块磁盘上，每个同完全相同的东西，这个不是必须的。-->
<!--datanode也可以指定多块磁盘，设置dfs.data.dir属性，往不同的地方存，并发时效率高，各个磁盘并不存相同的东西，只是为了方便写操作-->
<property>
<name>dfs.name.dir</name>
<value>/home/hadoop/name1,/home/hadoop/name2</value>
</property>


mv mapred-site.xml.example vim mapred-site.xml
<property>
<name>mapreduce.framework.name</name>
<value>yarn</value>
</property>

在yarn-site.xml中<configuration>里加入：
<property>
<name>yarn.resourcemanager.hostname</name>
<value>s1</value>
</property>
<property>
<name>yarn.nodemanager.aux-services</name>
<value>mapreduce_shuffle</value>
</property>

#将hadoop添加到环境变量：
sudo vim /etc/profile
export HADOOP_HOME=/home/hadoop/apps/hadoop-2.6.4
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
拷到其他机器
sudo scp /etc/profile s2:/etc/
sudo scp /etc/profile s3:/etc/
sudo scp /etc/profile s4:/etc/


#格式化namenode
hadoop namenode -format
怎样算格式化成功呢？
-执行完的时候有提示 /home/hadoop/hdpdata/dfs/name has been successfully formatted.

#启动：
/home/hadoop/apps/hadoop-2.6.4/sbin/hadoop-daemon.sh start namenode
也可以直接hadoop-daemon.sh start namenode ,因为已经加了环境变量把sbin加进去了
然后用jps查看看是否有NameNode
通过网页访问(hadoop里内置了一个小的web应用服务器[jetty服务器]，他的默认端口是50070,http://s1:50070)
查看当前有多少空间，通过点击上方的overview标签查，地址为http://s1:50070/dfshealth.html#tab-overview


#####hadoop用户的免密登录
ssh-kengen
ssh-copy-id -i ~/.ssh/id_rsa.pub s1
ssh-copy-id -i ~/.ssh/id_rsa.pub s2
ssh-copy-id -i ~/.ssh/id_rsa.pub s3
ssh-copy-id -i ~/.ssh/id_rsa.pub s4

#把hadoop放到其他机器
cd /home/hadoop
tar -vczf apps.tar.gz apps
scp /home/hadoop/apps.tar.gz s2:/home/hadoop
ssh s2 tar -vxzf /home/hadoop/apps.tar.gz
scp /home/hadoop/apps.tar.gz s3:/home/hadoop
ssh s3 tar -vxzf /home/hadoop/apps.tar.gz
scp /home/hadoop/apps.tar.gz s4:/home/hadoop
ssh s4 tar -vxzf /home/hadoop/apps.tar.gz

#启动datanode，在其他机器上
hadoop-daemon.sh start datanode
hadoop-daemon.sh stop datanode  停止服务
#其他机器的datanode是怎么跟本机的namenode握手的呢？是因为hadoop目录连同配置文件一起拷过去了，他们都认识namenode为s1
日志文件是在：/home/hadoop/apps/hadoop-2.6.4/logs/目录下的 那个.log文件，而不是.out文件
namenode机器上jps查看，有namenode,还有个secondaryNameNode(这个就是想本机的0.0.0.0,这个慢，也可以在hdfs-site.xml中加dfs.secondary.http.address这个属性为s1:50090)
而datanode机器上jps查看只有datanode

#批量启动：
sbin/start-all.sh起所有(不建议)   start-dsf.sh只起dsf     start-yarn.sh只起yarn
批量启动哪些？在etc/hadoop/slaves中，这文件纯粹是给批量启动用的
vim slaves里加入（默认里面有个localhost，把这个去掉，改成底下的）
s2
s3
s4

#批量停止
sbin/stop-dsf.sh

#启动yarn
sbin/yarn-daemon.sh start resourcemanager
其他机器
sbin/yarn-daemon.sh start nodemanager


##### hadoop shell命令操作
hadoop fs -ls /     查看   或者  hdfs dfs -ls /
hadoop fs -put 1.txt /     上传到hdfs的根目录下
hadoop fs -get /1.txt      下载文件保存到当前目录下
hadoop fs -cat /1.txt      读取文件根目录下的1.txt
hadoop fs -mkdir -p /wordcount/input   在根目录下创建一个二级目录
hadoop fs -put a.txt b.txt /wordcount/input     上传多个文件
hdfs dfsadmin -report     查看datanode节点的情况，跟在s1:9000这个web上看到的是一样的，但会比他更实时准确
其他命令：
moveFromLocal 从本地剪切到hdfs  moveFromLocal a.txt /hd/
moveToLocal  从hdfs剪切到本地  moveToLocal /hd/a.txt /home/hadoop
appendToFile 追加一个文件到hdfs某个文件的末尾
getmerge     合并hdfs里的文件
rm           删除文件或文件夹
hadoop fs -du -s -h hdfs://s1:9000/*   统计文件夹的大小信息
hadoop fs -count /aaa/               统计指定目录下的文件节点数量
hadoop fs -setrep 3 /1.txt           改变a.txt这个文件的副本数（默认2个，在配置文件里配置的2个）

## hadoop目录所在：
cd /home/hadoop/hdpdata/dfs/data/current/BP-xxxxx/current/finalized/subdir0/subdir0 这样的目录下
的 blk_xxx文件，默认128M才会切成多个块，可以直接cat blk_xxx来查看刚才保存的1.txt的内容。
同时在其他机器上也有这样的文件。

#### 执行mapreduce去统计单词wordcount
hadoop jar /home/hadoop/apps/hadoop-2.6.4/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.4.jar wordcount /wordcount/input /wordcount/output
--其中wordcount是要执行的这个包里的方法名，这方法需要两参数：输入和输出，要求output这个目录不存在，要是存在的话会报错
可以查看计算的结果
hadoop fs -cat /wordcount/output/part-r-00000


################### 讲解NameNode和SecondaryNameNode的关系
首先明白两个概念：
1、edits是记录操作日志的
2、fsimage是内存落盘到的目录，也就是镜像文件。
应该定期把edits和fsimage合并一下，生成新的fsimage，这样以后合并的时候不至于特别慢。
合并完后，要删掉edits或至少只留下未合并的edits。
正在写的edits是edits_inprogress，非inprogress的edits是非正在写的。
这个事不能在namenode上去做，而是要在secondnameNode上做。
namenode会定期(默认是30分钟)或edits到达一定的量时，会发请求给secondnameName，让他来合并。
secondnamenode会把namenode里的fsimage和一大堆edits下载到他所在的机器，
加载fsimage到内存，根据元数据生成的算法，在内存中重放一遍，再把内存dump到fsimage.chkpoint中。
上传到namenode服务器，并请求覆盖原来的fsimage（rename fsimage.checkpoint为fsimage）

########
hdpdata目录下的dfs有两个子目录，name和namesecondary，如果name被删除了，也可以拿namesecondary复制一份命名为name，也可以再用，只是会丢失一些最新的没来的及落盘的数据。

###########datanode的默认超时时间是10分30秒。(掉线或kill掉namenode)
由这两参数确定heartbeat.recheck.interval 默认为5分钟，里面是存毫秒单位
dfs.heartbeat.interval  默认为3，存的是秒单位
超时时间的计算公式是 2*heartbeat.recheck.interval + 10*dfs.heartbeat.interval

########### hadoop开发要引入的包
我把从apache官网上下载下来的hadoop包里的share/hadoop目录拷贝出来到跟项目平级的目录叫 lib_third
在idea工具中，File->Project Structure里Libraies中加入jar包，有如下是需要加入的：
1、lib_third/hadoop/common目录里的hadoop-common-2.6.4.jar  及它底下lib里的所有。
2、lib_third/hadoop/hdfs目录里的hadoop-hdfs-2.6.4.jar  及它底下lib里的所有。
3、lib_third/hadoop/mapreduce目录里的除hadoop-mapreduce-examples-2.6.4.jar之外的所有  及它底下lib里的所有。
4、lib_third/hadoop/yarn目录里的除包含server之外的所有  及它底下lib里的所有。

########### 用程序实现统计单词（MapReduces）
在服务器上任意s1,s2,s3,s4上执行：
hadoop fs -mkdir -p /wordcount/input
hadoop fs -rm -r /wordcount/output  需要删除output，不然会报错
# 把这几个现成的文件上传到hdfs
cd /home/hadoop/apps/hadoop-2.6.4
hadoop fs -put NOTICE.txt LICENSE.txt README.txt /wordcount/input
# 执行打好包的jar   java -cp jar包名 主类名 参数1 参数2（参数是在主类main中指定要接收的参数）
java -cp wordcount.jar cn.jhsoft.bigdata.hadoop.mr.wcdemo.WordcountDriver /wordcount/input /wordcount/output
#但是上面这样执行，如果wordcount.jar里没包含hadoop那一大堆包的话，就会报错，需要下面这样执行.它会把所有hadoop依赖包含进来。
hadoop jar wordcount.jar cn.jhsoft.bigdata.hadoop.mr.wcdemo.WordcountDriver /wordcount/input /wordcount/output

############ MapReduces分区，也就是结果按不同的要求生成统计结果
MapReduces里有个组件叫Partition，他是负责Map Task跑的时候，哪些单词进哪个分区，（后面一步就是分区里的进Reduces）
他的默认实现原理是，把key也就是单词进行hashCode再取模。取模是有几个Reduces就 %几，这样保证一个Reduces得到一个分区。
如果流量统计项目中，结果想按省来分出不同的输出结果，或者说在单词统计项目中，想要A-F一个统计文件，F-G一个文件，就需要对Partition组件进行重写。

#### MapReduces其他
number of split : 3 是指map有三个切片。其实大概就是 /wordcount/input里有几个文件大概就是几个切片。当然还得根据minBlock和maxBlock来。
MapReduces可以通过web页面来查看他的执行情况：http://s1:8088/

<<<<<<< HEAD
=======
############ MapReduces中，Combiner组件的作用：
在Map Task进行时，生成<单词a,数量1>，<a,1>,<a,1>,<b,1>,<b,1>这样一个一个，很费空间还有传输的时候也影响速度。
Combiner组件的任务是在使这样的生成，进行合并，如<a,3>,<b,2>，实质上Combiner也是继承自Reducer，此类的实现与WordcountReducer完全相同，所以可以用它来作为Combiner。
但这也只适合能合并的情况，如果是一些运算类的，不合适合并就不要在这合并了，还是像原来一样在最后合并。

############ MapReduces合适处理大文件，最好hdfs里存储的就是大文件，怎么弄？
->这在数据源那进行控制，如把多个日志合并了，再往hdfs里写。但是如果是一些老数据，或无法合并，怎么弄？
小文件性能差，因为一个小文件就会有一个切片
->那就用job.setInputFormatClass(CombineTextInputFormat.class);在处理的时候进行逻辑合并。
这样就能少几个切片，

########### MapReduces运行时，Job所需的资源有：job.split, job.xml, jar这三个文件。

########### MapReduces数据倾斜
在做订单与商品做join on 的时候，发现有这样一个问题，因为他是拿 商品ID(pid)作为key的，这样在Reduces Task的时候，如果一个商品的订单特别多，如订单有10000个，而另外的商品只有2个订单。
这样因为默认 MapReduces是用key来hash来确定他是在哪个Reduces上跑。就导致一个Reduces特别忙，另外一个特别闲，这样就特别不均衡。就叫数据倾斜。
-》如何解决？
思路就是在Map里处理完，不到Reduces，Map里是按切片来的，还算很均衡。
订单表数据多，商品表数据少。这样就可以在Map Task里，每条订单，去MapReduces提供的缓存里取数据。取完直接拼接上，就出结果了，不用经过Reduce Task。
******* 这个就是通过 DistributedCache[分布式的缓存]来解决。
******* hadoop在本地跑出来的目录是 /tmp/hadoop-chen/mapred/local/xxx

########### MapReduces所有项目的思路：：
****先想清楚谁是key,想到相同的key会进同一个Reduces

########### MapReduces想把结果写到数据库或自定义结果文件的文件名：：用自定义job.setOutputFormatClass
########### 关于小文件的问题，之前想到两种办法，一种是在数据集成之前先弄成大文件，再放到hdfs，另外一种方法job.setInputFormatClass(CombineTextInputFormat.class);来合并
还有一种办法是：用MapReduces，把小文件弄成k,v形式，k为文件名，v为文件内容。实例代码在 cn.jhsoft.bigdata.hadoop.mr.combinefile 包中。

########### MapReduces全局计数器：每台机器每个Task，这里会全局来计数：（logenhance项目中有用到）
// 获取一个计数器用来记录不合法的日志行数, 组名malformed, 计数器名称malformedline
Counter counter = context.getCounter("malformed", "malformedline");
counter.increment(1); // 计数器加1
或者用枚举形式定义计数器，建议用上面的的字符串形式
enum MyCounter {MALFORORMED, NORMAL}
context.getCounter(MyCounter.MALFORORMED).increment(1);

########### MapReduces优化参数：
shuffle性能优化：
mapreduce.task.io.sort.mb   100    //shuffle的环形缓冲区大小，默认100m,可以调大
mapreduce.map.sort.spill.percent  0.8    // 环形缓冲区溢出的闸值，默认80%




########################################################################################
########################################################################################
### Hive安装
########################################################################################
########################################################################################
### hive装一台或多台都行，不需要集群，连同一个mysql就行。
1、apache-hive-1.2.1-bin.tar.gz
2、解压到apps目录里
cd /home/hadoop
tar -vxzf apache-hive-1.2.1-bin.tar.gz -C apps
3、配置文件
cd apps
mv apache-hive-1.2.1-bin/ hive

cd hive
# 创建一个新的配置文件：
vi conf/hive-site.xml
加入：
<configuration>
<property>
<name>javax.jdo.option.ConnectionURL</name>
<value>jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true</value>
<description>JDBC connect string for a JDBC metastore</description>
</property>

<property>
<name>javax.jdo.option.ConnectionDriverName</name>
<value>com.mysql.jdbc.Driver</value>
<description>Driver class name for a JDBC metastore</description>
</property>

<property>
<name>javax.jdo.option.ConnectionUserName</name>
<value>root</value>
<description>username to use against metastore database</description>
</property>

<property>
<name>javax.jdo.option.ConnectionPassword</name>
<value>123456</value>
<description>password to use against metastore database</description>
</property>
</configuration>

#### 还需要把mysql-connector-java-5.1.28.jar包放到 hive/lib目录下

4、启动
/home/hadoop/apps/hive/bin/hive

5、启动问题：
Jline包版本不一致的问题，需要拷贝hive的lib目录中jline.2.12.jar的jar包替换掉hadoop中的
cp /home/hadoop/apps/hive/lib/jline-2.12.jar /home/hadoop/apps/hadoop-2.6.4/share/hadoop/yarn/lib/jline-0.9.94.jar

****创建了库之后，在hdfs的网页上就能看到 数据库的目录了 也就是在hdfs根目录下的这个目录下  /user/hive/warehouse
****在mysql中也能看到新建了个hive库，里面有DBS表和TBLS表，能看到自己建的库表。

5.1 测试
create database jhsoft;
use jhsoft;
create table test(id bigint, username string) row format delimited fields terminated by ',';
然后在hdfs里put 文件到 /user/hive/warehouse/jhsoft.db/test 下。
如果建表出错：FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:For direct MetaStore DB connections, we don't support retries at the client level.)
需要在mysql中   alter database hive character set latin1;

6.建表(默认是内部表)
create table trade_detail(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\t';
#建分区表
create table td_part(id bigint, account string, income double, expenses double, time string) partitioned by (logdate string) row format delimited fields terminated by '\t';
#建外部表
create external table td_ext(id bigint, account string, income double, expenses double, time string) row format delimited fields terminated by '\t' location '/td_ext';

7.创建分区表
#普通表和分区表区别：有大量数据增加的需要建分区表
create table book (id bigint, name string) partitioned by (pubdate string) row format delimited fields terminated by '\t';

#分区表加载数据
load data local inpath './book.txt' overwrite into table book partition (pubdate='2010-08-22');
load data local inpath '/root/data.am' into table beauty partition (nation="USA");
select nation, avg(size) from beauties group by nation order by avg(size);

8、启动hive为服务：/home/hadoop/apps/hive/bin/hiveserver2
在另外一个终端用命令行登录启动的服务：
/home/hadoop/apps/hive/bin/beeline
在命令行里输入：     !connect jdbc:hive2://localhost:10000      默认端口是10000
用户名输入  hadoop      这个可以在set里修改，一般情况下默认就是这个hadoop也不需要修改。
密码输入空    进去的操作就跟在mysql命令行一样。
>>>>>>> 834e354393bcbc0269e0d87be3d3cbf5ecde19bf
